{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "219dff62",
   "metadata": {},
   "source": [
    "This notebook contains the bulk of the code for this project. It creates several kubeflow pipelines components, some custom, some based on Vertex AI's ready-made components to integrate with google cloud. This notebook should only need to be run once; after that, setting up the cloud function will fully automate the process to run on its own when a new foundational csv is uploaded (for example, if you wanted to get new predictions each week of the NFL season).\n",
    "\n",
    "Please note that this notebook can be repurposed for others' projects. Preprocessing and model-building would need to be re-defined to fit the new data, but the infrastructure to automate a full modeling pipeline kicked off by uploading a beginning csv will remain. \n",
    "\n",
    "Further, in the cases of hard-coded paths, one would need to change these to align with their own google cloud setup.\n",
    "\n",
    "Lastly, the purpose of this project was to build out the infrastructure for an automated modeling process kicked off by introducing new data; its purpose is NOT to build the most performant model possible. As such, any model predictions should not be considered a gambling recommendation. No bets should be made based on its output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d91e495",
   "metadata": {},
   "source": [
    "INSTALL PACKAGES USED FOR PIPELINE CONSTRUCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e95c40-e098-4f29-a0e0-810b2c1e9dd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install kfp --pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b9072f-c1b7-4f7a-8a24-7f8bf446d780",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python3 -m pip install --upgrade kfp --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a45503f-407d-4ee5-bcc4-c79c4aa967fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "USER_FLAG = \"--user\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4945e6-58fe-4941-ad55-2a6eaf4101fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip3 install {USER_FLAG} google-cloud-aiplatform --upgrade\n",
    "!pip3 install {USER_FLAG} kfp google-cloud-pipeline-components==0.1.1 --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471a632e-4b5e-4858-95c1-c43cee7450bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# AND IMPORT THEM\n",
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import (Artifact, Dataset, Input, InputPath, Model, Output,\n",
    "                        OutputPath, ClassificationMetrics, Metrics, component)\n",
    "from kfp.components import create_component_from_func\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "from google_cloud_pipeline_components import aiplatform as gcc_aip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb36ebc-def5-4f8c-8496-6c2ed816fc18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SERVICE_ACCOUNT = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06c955a-9dcf-44e1-95b6-c12d70ee596a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# REPURPOSED FROM GOOGLE'S QWIKLABS TUTORIALS\n",
    "import os\n",
    "import sys\n",
    "\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "if (\n",
    "    SERVICE_ACCOUNT == \"\"\n",
    "    or SERVICE_ACCOUNT is None\n",
    "    or SERVICE_ACCOUNT == \"[your-service-account]\"\n",
    "):\n",
    "    # Get your service account from gcloud\n",
    "    if not IS_COLAB:\n",
    "        shell_output = !gcloud auth list 2>/dev/null\n",
    "        SERVICE_ACCOUNT = shell_output[2].replace(\"*\", \"\").strip()\n",
    "\n",
    "    if IS_COLAB:\n",
    "        shell_output = ! gcloud projects describe  $PROJECT_ID\n",
    "        project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
    "        SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
    "\n",
    "print(\"Service Account:\", SERVICE_ACCOUNT)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e782ee0a-76d3-48ed-af13-4a71eeb5b367",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DEFINE VARIABLES FOR GOOGLE CLOUD INTEGRATION - CHANGE TO MATCH OWN BUCKET NAMES\n",
    "REGION = 'us-central1' # NEEDS TO BE SAME REGION AS BUCKETS\n",
    "PROJECT_ID = !(gcloud config get-value core/project)\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "BUCKET = 'gs://nfl_spreads_model_output_gamma' # FOR PIPELINE OUTPUT - MUST CREATE BUCKET TO RUN\n",
    "INPUT_BUCKET = 'gs://nfl_spreads_central' # BUCKET HOSTING SPREADS DATA - MUST CREATE AND UPLOAD CSV TO RUN\n",
    "PIPELINE_ROOT = f\"{BUCKET}/pipeline_root/\"\n",
    "PIPELINE_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8207b0a0-8228-49b0-ba19-e5ba08327131",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CREATING CUSTOM KUBEFLOW COMPONENT\n",
    "# CORE GOAL IS TO REFORMAT ORIGINAL DATASET TO BE MACHINE READABLE FOR MODEL, SPLIT TO TRAIN/SERVE GROUPS\n",
    "# BOTH DATASETS BE PASSED AS AN ARTIFACT TO TRAINING COMPONENT\n",
    "\n",
    "@kfp.v2.dsl.component(\n",
    "    # PACKAGES NECESSARY FOR COMPONENT TO RUN\n",
    "    packages_to_install = [\n",
    "        'pandas', \n",
    "        'numpy', \n",
    "        'scikit-learn', \n",
    "        'tensorflow', \n",
    "        'gcsfs', \n",
    "        'fsspec']\n",
    ")\n",
    "def preprocess_raw_data(\n",
    "    input_file: Input[Dataset], # INPUT FILE WILL BE FED BY IMPORTER NODE IN PIPELINE DEFINITION\n",
    "    dataset: Output[Dataset], # TRAINING DATASET OUTPUT\n",
    "    preds_dataset: Output[Dataset] # SERVING DATASET OUTPUT\n",
    "): \n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import re\n",
    "    from sklearn.preprocessing import StandardScaler, normalize\n",
    "    import os\n",
    "    import tensorflow as tf\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "    \n",
    "    # READ INPUT FILE IN AS PANDAS DATAFRAME\n",
    "    # ADDITIONAL DOCUMENTATION ON PASSING KUBEFLOW ARTIFACTS B/W COMPONENTS HERE: \n",
    "    # https://www.kubeflow.org/docs/components/pipelines/v2/data-types/artifacts/\n",
    "    df = pd.read_csv(input_file.path) \n",
    "    \n",
    "\n",
    "    # FIRST YEAR W/SPREAD, ONLY 1 GAME IN RECORDS; TOSSING\n",
    "    df = df[df['schedule_season'] != 1978]\n",
    "\n",
    "    # ONLY WANT REGULAR SEASON; NOT DEALING WITH PICK EMS INITIALLY\n",
    "    df = df[~df['schedule_week'].isin(['Wildcard', 'Division', 'Conference', 'Superbowl'])]\n",
    "    df = df[~df['team_favorite_id'].isna()]\n",
    "\n",
    "    # MARGIN = FAVORITE'S SCORE - DOG'S SCORE\n",
    "    df['margin'] = np.where(df['team_home'] == df['team_favorite_id'],\\\n",
    "                            df['score_home'] - df['score_away'],\\\n",
    "                            df['score_away'] - df['score_home'])\n",
    "\n",
    "    # NY GAMBLING (AT LEAST MY APP) CAN'T BET PUSH - GIVES BACK ORIGINAL BET IF IT HAPPENS\n",
    "    # SO NOT GOING TO MODEL FOR PUSH OUTCOMES\n",
    "    df = df[df['margin'] != df['spread_favorite']]\n",
    "    df = df.reset_index()\n",
    "\n",
    "    # DID THE FAVORITE BEAT THE SPREAD?\n",
    "    df['beat_spread'] = np.where(df['margin'] > df['spread_favorite'],\\\n",
    "                            1,\\\n",
    "                            0)\n",
    "\n",
    "    # WANT A MONTH OF YEAR VAR; SOME MINOR STRING FORMATTING\n",
    "    df['month'] = df['schedule_date'].apply(lambda x: re.sub('/',  '', x[:2]))\n",
    "    \n",
    "    # AND WHETHER HOME TEAM WON OR NOT\n",
    "    df['home_win'] = np.where(df['score_home'] > df['score_away'], \\\n",
    "                              1,\\\n",
    "                              0)\n",
    "\n",
    "    df['month'] = df['month'].astype(int)\n",
    "    df['schedule_week'] = df['schedule_week'].astype(int)\n",
    "    df['over_under_line'] = df['over_under_line'].astype(float)\n",
    "\n",
    "    # PLACEHOLDER VALS FOR CURRENT WIN TOTALS/LAST YEAR'S WIN TOTALS FOR EACH TEAM\n",
    "    df['away_season_wins'] = None\n",
    "    df['away_season_wins_yr_prior'] = None\n",
    "    df['home_season_wins'] = None\n",
    "    df['home_season_wins_yr_prior'] = None\n",
    "\n",
    "    # ACTUALLY DEFINE THEM\n",
    "    for i in range(len(df)):\n",
    "        wk = df.loc[i, 'schedule_week']\n",
    "        yr = df.loc[i, 'schedule_season']\n",
    "        awy_tm = df.loc[i, 'team_away']\n",
    "        hm_tm = df.loc[i, 'team_home']\n",
    "        \n",
    "        # DEFINE CURRENT SEASON AWAY TEAM WIN TOTAL = NUM OF OBSERVATIONS FITTING \"THEY WON\" CRITERIA\n",
    "        df.loc[i, 'away_season_wins'] = len(df[\n",
    "                                        # SAME YEAR\n",
    "                                        (df['schedule_season'] == yr) &\n",
    "                                        # EARLIER WEEK\n",
    "                                        (df['schedule_week'] < wk) &\\\n",
    "                                        (    # WAS AWAY TEAM, HOME TEAM DIDN'T WIN\n",
    "                                            ((df['team_away'] == awy_tm) & (df['home_win'] == 0)) |\\\n",
    "                                            # WAS HOME TEAM, HOME TEAM DID WIN\n",
    "                                            ((df['team_home'] == awy_tm) & (df['home_win'] == 1))\n",
    "                                        )\n",
    "                                       ])\n",
    "        \n",
    "        # LAST YEAR'S WIN TOTAL\n",
    "        df.loc[i, 'away_season_wins_yr_prior'] = len(df[\n",
    "                                                 (df['schedule_season'] == yr - 1) &\\\n",
    "                                                 (\n",
    "                                                     ((df['team_away'] == awy_tm) & (df['home_win'] == 0)) |\\\n",
    "                                                     ((df['team_home'] == awy_tm) & (df['home_win'] == 1))\n",
    "                                                 )\n",
    "                                                ])\n",
    "        \n",
    "        # CURRENT WINS FOR HOME TEAM\n",
    "        df.loc[i, 'home_season_wins'] = len(df[\n",
    "                                        (df['schedule_season'] == yr) &\\\n",
    "                                        (df['schedule_week'] < wk) &\\\n",
    "                                        (\n",
    "                                            ((df['team_away'] == hm_tm) & (df['home_win'] == 0)) |\\\n",
    "                                            ((df['team_home'] == hm_tm) & (df['home_win'] == 1))\n",
    "                                        )\n",
    "                                       ])\n",
    "        \n",
    "        # HOME TEAM LAST YEAR\n",
    "        df.loc[i, 'home_season_wins_yr_prior'] = len(df[\n",
    "                                                 (df['schedule_season'] == yr - 1) &\\\n",
    "                                                 (\n",
    "                                                     ((df['team_away'] == hm_tm) & (df['home_win'] == 0)) |\\\n",
    "                                                     ((df['team_home'] == hm_tm) & (df['home_win'] == 1))\n",
    "                                                 )\n",
    "                                                ])\n",
    "\n",
    "    df['away_season_wins'] = df['away_season_wins'].astype(int)\n",
    "    df['away_season_wins_yr_prior'] = df['away_season_wins_yr_prior'].astype(int)\n",
    "    df['home_season_wins'] = df['home_season_wins'].astype(int)\n",
    "    df['home_season_wins_yr_prior'] = df['home_season_wins_yr_prior'].astype(int)\n",
    "\n",
    "    # ONE-HOT ENCODE CATEGORICALS\n",
    "    df = pd.get_dummies(df, prefix = ['home', 'away'], columns = ['team_home', 'team_away'] )\n",
    "\n",
    "    # DROP COLUMNS I DON'T EXPECT TO EVER BE IN IT\n",
    "    df_model = df.drop(columns = ['index', 'schedule_date', 'schedule_playoff', 'team_favorite_id',\\\n",
    "                                  'score_home', 'score_away', 'stadium', 'weather_detail', 'margin', 'home_win'])\n",
    "\n",
    "    #DROP COLUMNS I THINK WILL BE HARD TO KNOW AT BETTING TIME (I.E. WEATHER) W/OUT ADDITIONAL DATA\n",
    "    df_model = df_model.drop(columns = ['weather_temperature', 'weather_wind_mph', 'weather_humidity'])\n",
    "\n",
    "    std_scl = StandardScaler()\n",
    "\n",
    "    # TRAIN, SERVE SPLIT\n",
    "    # SERVING FOR LAST 3 WEEKS IN DATASET (IN FUTURE MAY DO JUST ONE WEEK - FOR NOW PREFER LARGER SAMPLE FOR TESTING)\n",
    "    # REST IS TRAINING\n",
    "    \n",
    "    latest_season = max(df_model['schedule_season'].values.tolist())\n",
    "    latest_week_of_latest_season = max(df_model[df_model['schedule_season'] == latest_season]['schedule_week'].values.tolist())\n",
    "    latest_week_of_int = latest_week_of_latest_season - 2\n",
    "    \n",
    "    serve = df_model[(df_model['schedule_season'] == latest_season) & (df_model['schedule_week'] >= latest_week_of_int)]\n",
    "    train = df_model[~df_model.index.isin(serve.index)]\n",
    "\n",
    "    # X, Y SPLIT\n",
    "    ex_train = train.drop(columns = ['beat_spread'])\n",
    "    why_train = train.drop(columns = [x for x in train.columns if x != 'beat_spread'])\n",
    "\n",
    "    ex_serve = serve.drop(columns = ['beat_spread'])\n",
    "    why_serve = serve.drop(columns = [x for x in df_model.columns if x != 'beat_spread'])\n",
    "\n",
    "    #EX TRANSFORMATIONS\n",
    "    ex_serve_scaled = std_scl.fit_transform(ex_serve)\n",
    "    ex_serve_norm = normalize(ex_serve)\n",
    "\n",
    "    ex_train_scaled = std_scl.fit_transform(ex_train)\n",
    "    ex_train_norm = normalize(ex_train)\n",
    "\n",
    "    # CONVERT BOOLS TO NUMBERS FOR EASIER CSV TRANSFERS\n",
    "    for col in ex_train.columns:\n",
    "        if False in ex_train[col].values.tolist():\n",
    "            train[col] = train[col].apply(lambda x: 1 if x == True else 0)\n",
    "            serve[col] = serve[col].apply(lambda x: 1 if x == True else 0)\n",
    "\n",
    "    # MAKE SURE ALL NUMERIC\n",
    "    for col in ex_train.columns:\n",
    "        train[col] = train[col].astype('float')\n",
    "        serve[col] = serve[col].astype('float')\n",
    "\n",
    "    train.to_csv(f\"{dataset.path}.csv\", index = False) # PASS ON TRAINING DATA ARTIFACT\n",
    "    serve.to_csv(f\"{preds_dataset.path}.csv\", index = False) # AND SERVING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3d4aac-a5ce-41b3-9abb-31720859c90e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SECOND CUSTOM KUBEFLOW COMPONENT - THIS ONE TO BUILD MODEL, BATCH PREDICT ON SERVING DATA\n",
    "# PLEASE NOTE, MY FOCUS IS PIPELINE INFRASTRUCTURE, NOT MODEL PERFORMANCE; NO MODEL OPTIMIZATION WORK HAS BEEN DONE\n",
    "# NO ONE SHOULD USE THIS APPLICATION FOR ACTUAL GAMBLING PURPOSES\n",
    "\n",
    "@kfp.v2.dsl.component(\n",
    "    packages_to_install = [\n",
    "        'tensorflow', \n",
    "        'pandas', \n",
    "        'joblib']\n",
    ")\n",
    "def build_model(\n",
    "    ds: Input[Dataset], # TRAINING DATASET PASSED FROM PREPROCESS COMPONENT\n",
    "    input_file: Input[Dataset], # SERVING DATASET PASSED FROM PREPROCESS COMPONENT\n",
    "    epcs : int, # EPOCHS TO TRAIN SIMPLE NN MODEL\n",
    "    output_model: Output[Model], # OUTPUT A MODEL TO SERVE TO AN ENDPOINT\n",
    "    prediction_dataset: Output[Dataset] # AND OUTPUT A CSV WITH BATCH PREDICTIONS\n",
    "    # NOTE - BOTH DEPLOYING A MODEL AND MAKING BATCH PREDICTIONS IS FAIRLY ATYPICAL\n",
    "    # I DID SO TO RETAIN THE FUNCTIONALITY TO DO EITHER IF REPURPOSING THIS CODE FOR OTHER PROJECTS\n",
    "):\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.layers import Dense, Flatten, Input, DenseFeatures\n",
    "    import pandas as pd\n",
    "    import os\n",
    "    import joblib\n",
    "    import numpy as np\n",
    "    \n",
    "    \n",
    "    train = pd.read_csv(f\"{ds.path}.csv\")\n",
    "    batch_size = 32\n",
    "\n",
    "    # The `tf.data.experimental.make_csv_dataset()` method reads CSV files into a dataset\n",
    "    train_ds = tf.data.experimental.make_csv_dataset(\n",
    "        f\"{ds.path}.csv\", \n",
    "        batch_size,\n",
    "        column_names=train.columns.tolist(),\n",
    "        label_name='beat_spread',\n",
    "        num_epochs=1)\n",
    "\n",
    "    def pack_features_vector(features, labels):\n",
    "      \"\"\"Pack the features into a single array.\"\"\"\n",
    "    # Using `tf.stack` we can stack a list of rank-R tensors into one rank-(R+1) tensor.\n",
    "      features = tf.stack(list(features.values()), axis=1)\n",
    "      return features, labels\n",
    "\n",
    "    train_ds = train_ds.map(pack_features_vector)\n",
    "    \n",
    "    model = tf.keras.Sequential([\n",
    "    Dense(128, activation = 'relu', name = 'hidden_layer_1'),\n",
    "    Dense(64, activation = 'relu', name = 'hidden_layer_2'),\n",
    "    Dense(1, activation='sigmoid', name = 'pred_layer') \n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "    # REDUCE OVERFITTING AND SHORTEN TRAIN TIME\n",
    "    earlystopping_callback = tf.keras.callbacks.EarlyStopping(patience=2)\n",
    "\n",
    "    # PROTECT AGAINST VM RESTARTS\n",
    "    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath='./local-training/checkpoints',\n",
    "        save_weights_only=True,\n",
    "        monitor='val_loss',\n",
    "        mode='min')\n",
    "\n",
    "\n",
    "    model.fit(train_ds,\n",
    "              callbacks=[earlystopping_callback, checkpoint_callback],\n",
    "              epochs=epcs)\n",
    "    \n",
    "    model.save(output_model.path) # MODEL OUTPUT FOR DEPLOYMENT\n",
    "    \n",
    "    pred_df = pd.read_csv(f\"{input_file.path}.csv\")\n",
    "    batch_size = 32\n",
    "    \n",
    "    pred_ds = tf.data.experimental.make_csv_dataset(\n",
    "        f\"{input_file.path}.csv\", \n",
    "        batch_size,\n",
    "        column_names=pred_df.columns.tolist(),\n",
    "        label_name='beat_spread',\n",
    "        num_epochs=1,  #DON'T WANT TO REPEAT INDEFINITELY\n",
    "        shuffle = False)  #IMPORTANT TO RETAIN ORDER FOR APPENDING PREDICTIONS TO DATAFRAME)\n",
    "    \n",
    "    pred_ds = pred_ds.map(pack_features_vector)\n",
    "\n",
    "    counter = 0 # USE FOR INDEX\n",
    "    \n",
    "    # PLACE HOLDERS TO FILL W/ACTUAL VALUES\n",
    "    pred_df['prediction'] = None\n",
    "    pred_df['predict_prob'] = None\n",
    "\n",
    "    for input_vals, answer in pred_ds: # ITERATE THROUGH TF DATASET\n",
    "        for prediction in model.predict(x = input_vals): # PULL NN's PREDS\n",
    "            pred_df.loc[counter, 'prediction'] = round(prediction[0]) # APPEND BINARY PRED TO APPROPRIATE DF RECORD\n",
    "            pred_df.loc[counter, 'predict_prob'] = prediction[0] # AND FULL PROBABILITY IN CASE IT IS OF INTEREST\n",
    "            counter += 1 # INCREMENT INDEX COUNTER\n",
    "    \n",
    "    pred_df.to_csv(f\"{prediction_dataset.path}.csv\", index = False) # OUTPUT PREDS CSV AS AN ARTIFACT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aae4fe3-b3ab-440e-9d00-f5b355ef09ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# RELYING ON VERTEX AI CUSTOM COMPONENTS FOR DEPLOYING TO GOOGLE CLOUD\n",
    "# STRONG GC PIPELINE EXAMPLES (INCLUDING THIS COMPONENT) HERE:\n",
    "# https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/pipelines/kfp2_pipeline.ipynb\n",
    "\n",
    "@kfp.v2.dsl.component(\n",
    "    packages_to_install=[\"google-cloud-aiplatform==1.25.0\"],\n",
    ")\n",
    "def deploy_model(\n",
    "    model: Input[Model],\n",
    "    project_id: str,\n",
    "    vertex_endpoint: Output[Artifact],\n",
    "    vertex_model: Output[Model],\n",
    "):\n",
    "    \"\"\"Deploys model to Vertex AI Endpoint.\n",
    "\n",
    "    Args:\n",
    "        model: The model to deploy.\n",
    "        project_id: The project ID of the Vertex AI Endpoint.\n",
    "\n",
    "    Returns:\n",
    "        vertex_endpoint: The deployed Vertex AI Endpoint.\n",
    "        vertex_model: The deployed Vertex AI Model.\n",
    "    \"\"\"\n",
    "    from google.cloud import aiplatform\n",
    "\n",
    "    aiplatform.init(project=project_id)\n",
    "\n",
    "    deployed_model = aiplatform.Model.upload(\n",
    "        display_name=\"nfl-spreads-model\",\n",
    "        artifact_uri=model.uri,\n",
    "        serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-12:latest\",\n",
    "    )\n",
    "    endpoint = deployed_model.deploy(machine_type=\"n1-standard-4\")\n",
    "\n",
    "    vertex_endpoint.uri = endpoint.resource_name\n",
    "    vertex_model.uri = deployed_model.resource_name\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cb4569-a1d2-498c-9f7d-fa554dd22fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PULL COMPONENTS TOGETHER (E.G. DEFINING DEPENDENCIES) TO DEFINE PIPELINE\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name = 'nfl-spreads-model',\n",
    "    description = 'attempt to deploy end-to-end spreads model pipeline w/kubeflow components'\n",
    ")\n",
    "def spreads_pipeline():\n",
    "    importer_task = dsl.importer(\n",
    "        artifact_uri = f\"{INPUT_BUCKET}/spreadspoke_scores.csv\", # DEFINE FILE FOR IMPORTER NODE TO PULL\n",
    "        artifact_class = Dataset,\n",
    "        reimport = True\n",
    "    )\n",
    "    \n",
    "    # FEEDS INTO PREPROCESSING\n",
    "    preprocess_task = preprocess_raw_data(importer_task.output)\n",
    "   \n",
    "    # INTO MODEL BUILDING\n",
    "    training_task = build_model(preprocess_task.outputs['dataset'], preprocess_task.outputs['preds_dataset'], 2)\n",
    "    \n",
    "    # INTO MODEL DEPLOYING\n",
    "    _ = deploy_model(\n",
    "        project_id=PROJECT_ID,\n",
    "        model=training_task.outputs['output_model'],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e5e45e-99c4-495d-be4e-d919ebf98fcf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# COMPILE MODEL AND DEFINE JSON PATH THAT STORES PIPELINE BUILD INFO\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=spreads_pipeline,\n",
    "    package_path=\"nfl_spreads_model.json\" \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5b5f8f-e267-47c4-8c95-bc6fa5fd49e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# UNIQUE DISPLAY NAME THAT INCLUDING TIME OF RUN\n",
    "DISPLAY_NAME = 'nfl_spreads_gamma_12_14_8am'\n",
    "\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name = DISPLAY_NAME,\n",
    "    template_path = \"nfl_spreads_model.json\",\n",
    "    pipeline_root = PIPELINE_ROOT,\n",
    "    enable_caching = False,\n",
    ")\n",
    "\n",
    "# RUN PIPELINE TO GET PREDICTION CSV ARTIFACT AND DEPLOYED MODEL\n",
    "job.run(service_account = SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a9efb6-9849-4f9a-86b6-d3300e911e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MOVING LOCAL JSON TO CLOUD STORAGE FOR CLOUD FUNCTION PURPOSES\n",
    "!gsutil cp nfl_spreads_model.json gs://nfl_spreads_model_output_gamma/nfl_spreads_model.json"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m113",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m113"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
